{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae1bb710-41ae-4a56-8b1d-e7e9e96fb990",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9b35632-e6db-4b74-862f-2ad2d28d8cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import joblib\n",
    "import re\n",
    "import numpy as np\n",
    "import csv        \n",
    "import os\n",
    "# Change directory to the root of the folder (this script was launched from the subfolder python_scripts)\n",
    "# All utils presuppose that we are working from the root directory of the github folder\n",
    "os.chdir(\"../\")\n",
    "from tqdm.notebook import tqdm\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from datetime import datetime\n",
    "import jsonlines\n",
    "import pandas as pd\n",
    "import cld3\n",
    "from iso639 import languages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef38be8-7971-44ce-bc0d-c9726d11e534",
   "metadata": {},
   "source": [
    "# Collecting data from Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6496f64-f27b-4620-a443-8fda78562c19",
   "metadata": {},
   "source": [
    "Download corpus from\n",
    "https://api.semanticscholar.org/corpus/download/\n",
    "\n",
    "Release downloaded: 2022-01-01 release\n",
    "\n",
    "To download the corpus, before continuing with this notebook, from the root folder, run the following commands:\n",
    "\n",
    "```\n",
    "mkdir -p data\n",
    "mkdir -p data/semanticscholar\n",
    "mkdir -p data/semanticscholar/2022-01-01\n",
    "mkdir -p data/semanticscholar/2022-01-01/corpus\n",
    "cd data/semanticscholar/2022-01-01/corpus/\n",
    "wget https://s3-us-west-2.amazonaws.com/ai2-s2-research-public/open-corpus/2022-01-01/manifest.txt\n",
    "wget -B https://s3-us-west-2.amazonaws.com/ai2-s2-research-public/open-corpus/2022-01-01/ -i manifest.txt\n",
    "```\n",
    "\n",
    "*ACHTUNG: This corpus occupies 192GB*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cc6d164-6014-4283-adf4-3ed6e64fd45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_version = '2022-01-01'\n",
    "corpus_folder = os.path.join('./data/semanticscholar/', corpus_version, 'corpus') # where you have the corpus\n",
    "data_folder = os.path.join('./data/semanticscholar/', corpus_version, 'data') # where you save the data\n",
    "os.makedirs(data_folder, exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecc093fb-a9e1-4489-8965-2d958d1aa138",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numbers_list(string):\n",
    "    set_str_digits = set([str(digit) for digit in range(10)])\n",
    "    numbers_list = []\n",
    "    found_string = ''\n",
    "    alert = False\n",
    "    new_number = False\n",
    "    \n",
    "    for i,char in enumerate(string):\n",
    "        if char in set_str_digits:\n",
    "            new_number = True\n",
    "            found_string += char\n",
    "        elif new_number == True:\n",
    "            numbers_list.append(int(found_string))\n",
    "            new_number = False\n",
    "            found_string = ''\n",
    "    if new_number == True:\n",
    "        numbers_list.append(int(found_string))\n",
    "    return numbers_list\n",
    "\n",
    "def get_year_volume_issue_firstPage(paper):\n",
    "    try:\n",
    "        year = paper['year']\n",
    "    except:\n",
    "        year = -1\n",
    "    numbers_list = get_numbers_list(paper['journalVolume'])\n",
    "    try:\n",
    "        volume = numbers_list[0]\n",
    "    except:\n",
    "        volume = -1\n",
    "    try:\n",
    "        issue = numbers_list[1]\n",
    "    except:\n",
    "        issue = -1\n",
    "    numbers_list = get_numbers_list(paper['journalPages'])\n",
    "    try:\n",
    "        firstPage = numbers_list[0]\n",
    "    except:\n",
    "        firstPage = -1\n",
    "    \n",
    "    return (year,volume,issue,firstPage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a57d02fc-50c3-40ff-903a-58695d56ab42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prune text from punctuation and junk...\n",
    "wnl = WordNetLemmatizer()\n",
    "pattern = re.compile(r'\\B#\\w*[A-Za-z]+\\w*|\\b\\w*[A-Za-z]+\\w*', re.UNICODE)\n",
    "def lemmatize(doc):\n",
    "    '''\n",
    "        Takes a string doc and returns the list of words, without punctuation and junk\n",
    "    '''\n",
    "    l = [wnl.lemmatize(t) for t in pattern.findall(doc)]\n",
    "    return [w.lower() for w in l if len(w) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c260a82d-d05f-4fd0-8a86-bed4648db6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    raise KeyError\n",
    "    print('Trying to load collected data...')\n",
    "    with gzip.open(os.path.join(data_folder, 'all_fieldsOfStudy_singles_set.pkl.gz'), 'rb') as fp:\n",
    "        all_fieldsOfStudy_singles_set = joblib.load(fp)\n",
    "    with gzip.open(os.path.join(data_folder, 'all_fieldsOfStudy_couples_set.pkl.gz'), 'rb') as fp:\n",
    "        all_fieldsOfStudy_couples_set = joblib.load(fp)\n",
    "    with gzip.open(os.path.join(data_folder, 'all_fieldsOfStudy_tuples_set.pkl.gz'), 'rb') as fp:\n",
    "        all_fieldsOfStudy_tuples_set = joblib.load(fp)\n",
    "    with gzip.open(os.path.join(data_folder, 'count_by_fieldsOfStudy_tuple_all_journalName_dict.pkl.gz'), 'rb') as fp:\n",
    "        count_by_fieldsOfStudy_tuple_all_journalName_dict = joblib.load(fp)\n",
    "    with gzip.open(os.path.join(data_folder, 'count_by_fieldsOfStudy_couple_all_journalName_dict.pkl.gz'), 'rb') as fp:\n",
    "        count_by_fieldsOfStudy_couple_all_journalName_dict = joblib.load(fp)\n",
    "    with gzip.open(os.path.join(data_folder, 'count_by_fieldsOfStudy_single_all_journalName_dict.pkl.gz'), 'rb') as fp:\n",
    "        count_by_fieldsOfStudy_single_all_journalName_dict = joblib.load(fp)\n",
    "    with gzip.open(os.path.join(data_folder, 'count_all_journalName_dict.pkl.gz'), 'rb') as fp:\n",
    "        count_all_journalName_dict = joblib.load(fp)\n",
    "    with gzip.open(os.path.join(data_folder, 'sample_all_journalName_dict.pkl.gz'), 'rb') as fp:\n",
    "        sample_all_journalName_dict = joblib.load(fp)\n",
    "    with gzip.open(os.path.join(data_folder, 'all_journalName_set.pkl.gz'), 'rb') as fp:\n",
    "        all_journalName_set = joblib.load(fp)\n",
    "    with gzip.open(os.path.join(data_folder, 'all_titles_dict.pkl.gz'), 'rb') as fp:\n",
    "        all_titles_dict = joblib.load(fp)\n",
    "    with gzip.open(os.path.join(data_folder, 'all_journal_dict.pkl.gz'), 'rb') as fp:\n",
    "        all_journal_dict = joblib.load(fp)\n",
    "    with gzip.open(os.path.join(data_folder, 'all_fieldsOfStudy_dict.pkl.gz'), 'rb') as fp:\n",
    "        all_fieldsOfStudy_dict = joblib.load(fp)\n",
    "    with gzip.open(os.path.join(data_folder, 'all_year_volume_issue_firstPage_dict.pkl.gz'), 'rb') as fp:\n",
    "        all_year_volume_issue_firstPage_dict = joblib.load(fp)\n",
    "    with gzip.open(os.path.join(data_folder, 'all_paperId_by_journal_by_first_fieldsOfStudy.pkl.gz'), 'rb') as fp:\n",
    "        all_paperId_by_journal_by_first_fieldsOfStudy = joblib.load(fp)\n",
    "    with gzip.open(os.path.join(data_folder, 'all_paperId_by_author.pkl.gz'), 'rb') as fp:\n",
    "        all_paperId_by_author = joblib.load(fp)\n",
    "    with gzip.open(os.path.join(data_folder, 'count_papers_by_author.pkl.gz'), 'rb') as fp:\n",
    "        count_papers_by_author = joblib.load(fp)\n",
    "    with gzip.open(os.path.join(data_folder, 'count_papers_by_author_by_first_fieldsOfStudy.pkl.gz'), 'rb') as fp:\n",
    "        count_papers_by_author_by_first_fieldsOfStudy = joblib.load(fp)\n",
    "    print('Loaded all data')\n",
    "except:    \n",
    "    print('Collecting data...')\n",
    "    start = datetime.now()\n",
    "    sample_all_journalName_dict = {}\n",
    "    all_titles_dict = {}\n",
    "    all_fieldsOfStudy_dict = {}\n",
    "    all_journal_dict = {}\n",
    "    all_year_volume_issue_firstPage_dict = {}\n",
    "    all_paperId_by_journal_by_first_fieldsOfStudy = {}\n",
    "    all_paperId_by_author = {}\n",
    "    count_papers_by_author = {}\n",
    "    count_papers_by_author_by_first_fieldsOfStudy = {}\n",
    "    count_all_journalName_dict = {}\n",
    "    count_by_fieldsOfStudy_tuple_all_journalName_dict = {}\n",
    "    count_by_fieldsOfStudy_couple_all_journalName_dict = {}\n",
    "    count_by_fieldsOfStudy_single_all_journalName_dict = {}\n",
    "    all_fieldsOfStudy_tuples_set = set()\n",
    "    all_fieldsOfStudy_couples_set = set()\n",
    "    all_fieldsOfStudy_singles_set = set()\n",
    "    all_journalName_set = set()\n",
    "    for ID in tqdm(range(6000)):\n",
    "        filename = os.path.join(corpus_folder,'s2-corpus-%.3d.gz'%ID)\n",
    "        with gzip.open(filename, 'rb') as f:\n",
    "            for paper in jsonlines.Reader(f):\n",
    "                # each line of the file is a dictionary with the paper's info\n",
    "                paper_id = paper['id']\n",
    "                title = paper['title'].lower()\n",
    "                # ONLY CONSIDER THE ENGLISH ONES\n",
    "                language = cld3.get_language(title).language # detector.detect(title).lang # from googletrans import Translator # detector = Translator()\n",
    "#                 if language_prediction.is_reliable == True: # probability > 0.99:\n",
    "#                     language = language_prediction.language\n",
    "                if language != 'en':\n",
    "                    continue\n",
    "                language = languages.get(alpha2=language[:2]).name # \n",
    "                all_titles_dict[paper_id] = title\n",
    "                journalName = paper['journalName'].lower()\n",
    "                venue = paper['venue'].lower()\n",
    "                fieldsOfStudy = tuple(paper['fieldsOfStudy'])\n",
    "                if len(fieldsOfStudy) > 0:\n",
    "                    first_fieldsOfStudy = fieldsOfStudy[0]\n",
    "\n",
    "                for author_dict in paper['authors']:\n",
    "                    try: \n",
    "                        author_id = author_dict['ids'][0]\n",
    "                    except:\n",
    "                        continue\n",
    "                    if author_id not in all_paperId_by_author:\n",
    "                        all_paperId_by_author[author_id] = set([paper_id])\n",
    "                        count_papers_by_author[author_id] = 1\n",
    "                    else:\n",
    "                        all_paperId_by_author[author_id].add(paper_id)\n",
    "                        count_papers_by_author[author_id] += 1\n",
    "                    if len(fieldsOfStudy) > 0:\n",
    "                        if first_fieldsOfStudy not in count_papers_by_author_by_first_fieldsOfStudy:\n",
    "                            count_papers_by_author_by_first_fieldsOfStudy[first_fieldsOfStudy] = {author_id:1}\n",
    "                        elif author_id not in count_papers_by_author_by_first_fieldsOfStudy[first_fieldsOfStudy]:\n",
    "                            count_papers_by_author_by_first_fieldsOfStudy[first_fieldsOfStudy][author_id] = 1\n",
    "                        else:\n",
    "                            count_papers_by_author_by_first_fieldsOfStudy[first_fieldsOfStudy][author_id] += 1\n",
    "\n",
    "                all_year_volume_issue_firstPage_dict[paper_id] = get_year_volume_issue_firstPage(paper)\n",
    "    \n",
    "                if len(fieldsOfStudy) > 0:\n",
    "                    all_fieldsOfStudy_dict[paper_id] = fieldsOfStudy\n",
    "                    \n",
    "                if len(fieldsOfStudy) > 0 and (len(venue) > 0 or len(journalName) > 0):\n",
    "                    if (journalName,venue) not in sample_all_journalName_dict:\n",
    "                        sample_all_journalName_dict[(journalName,venue)] = paper.copy()\n",
    "                        count_all_journalName_dict[(journalName,venue)] = 1\n",
    "                        all_journalName_set.add((journalName,venue))\n",
    "                    else:\n",
    "                        count_all_journalName_dict[(journalName,venue)] += 1\n",
    "                    all_journal_dict[paper_id] = (journalName,venue)\n",
    "\n",
    "                    # TUPLE\n",
    "                    all_fieldsOfStudy_tuples_set.add(fieldsOfStudy)\n",
    "                    if fieldsOfStudy not in count_by_fieldsOfStudy_tuple_all_journalName_dict:\n",
    "                        count_by_fieldsOfStudy_tuple_all_journalName_dict[fieldsOfStudy] = {(journalName,venue):1}\n",
    "                    elif (journalName,venue) not in count_by_fieldsOfStudy_tuple_all_journalName_dict[fieldsOfStudy]:\n",
    "                        count_by_fieldsOfStudy_tuple_all_journalName_dict[fieldsOfStudy][(journalName,venue)] = 1\n",
    "                    else:\n",
    "                        count_by_fieldsOfStudy_tuple_all_journalName_dict[fieldsOfStudy][(journalName,venue)] += 1\n",
    "\n",
    "                    # COUPLE\n",
    "                    fieldsOfStudy = fieldsOfStudy[:2]\n",
    "                    all_fieldsOfStudy_couples_set.add(fieldsOfStudy)\n",
    "                    if fieldsOfStudy not in count_by_fieldsOfStudy_couple_all_journalName_dict:\n",
    "                        count_by_fieldsOfStudy_couple_all_journalName_dict[fieldsOfStudy] = {(journalName,venue):1}\n",
    "                    elif (journalName,venue) not in count_by_fieldsOfStudy_couple_all_journalName_dict[fieldsOfStudy]:\n",
    "                        count_by_fieldsOfStudy_couple_all_journalName_dict[fieldsOfStudy][(journalName,venue)] = 1\n",
    "                    else:\n",
    "                        count_by_fieldsOfStudy_couple_all_journalName_dict[fieldsOfStudy][(journalName,venue)] += 1\n",
    "\n",
    "                    # SINGLE\n",
    "                    fieldsOfStudy = fieldsOfStudy[0]\n",
    "                    all_fieldsOfStudy_singles_set.add(fieldsOfStudy)\n",
    "                    if fieldsOfStudy not in count_by_fieldsOfStudy_single_all_journalName_dict:\n",
    "                        count_by_fieldsOfStudy_single_all_journalName_dict[fieldsOfStudy] = {(journalName,venue):1}\n",
    "                        all_paperId_by_journal_by_first_fieldsOfStudy[fieldsOfStudy] = {(journalName,venue):set([paper_id])}\n",
    "                    elif (journalName,venue) not in count_by_fieldsOfStudy_single_all_journalName_dict[fieldsOfStudy]:\n",
    "                        count_by_fieldsOfStudy_single_all_journalName_dict[fieldsOfStudy][(journalName,venue)] = 1\n",
    "                        all_paperId_by_journal_by_first_fieldsOfStudy[fieldsOfStudy][(journalName,venue)] = set([paper_id])\n",
    "                    else:\n",
    "                        count_by_fieldsOfStudy_single_all_journalName_dict[fieldsOfStudy][(journalName,venue)] += 1\n",
    "                        all_paperId_by_journal_by_first_fieldsOfStudy[fieldsOfStudy][(journalName,venue)].add(paper_id)\n",
    "\n",
    "        end = datetime.now()\n",
    "\n",
    "        if (ID+1) % 1000 == 0:\n",
    "            print(f'Read {ID+1} files after {end-start}.',flush=True)\n",
    "\n",
    "    with gzip.open(os.path.join(data_folder, 'all_fieldsOfStudy_singles_set.pkl.gz'), 'wb') as fp:\n",
    "        joblib.dump(all_fieldsOfStudy_singles_set,fp)\n",
    "    with gzip.open(os.path.join(data_folder, 'all_fieldsOfStudy_couples_set.pkl.gz'), 'wb') as fp:\n",
    "        joblib.dump(all_fieldsOfStudy_couples_set,fp)\n",
    "    with gzip.open(os.path.join(data_folder, 'all_fieldsOfStudy_tuples_set.pkl.gz'), 'wb') as fp:\n",
    "        joblib.dump(all_fieldsOfStudy_tuples_set,fp)\n",
    "    with gzip.open(os.path.join(data_folder, 'count_by_fieldsOfStudy_tuple_all_journalName_dict.pkl.gz'), 'wb') as fp:\n",
    "        joblib.dump(count_by_fieldsOfStudy_tuple_all_journalName_dict,fp)\n",
    "    with gzip.open(os.path.join(data_folder, 'count_by_fieldsOfStudy_couple_all_journalName_dict.pkl.gz'), 'wb') as fp:\n",
    "        joblib.dump(count_by_fieldsOfStudy_couple_all_journalName_dict,fp)\n",
    "    with gzip.open(os.path.join(data_folder, 'count_by_fieldsOfStudy_single_all_journalName_dict.pkl.gz'), 'wb') as fp:\n",
    "        joblib.dump(count_by_fieldsOfStudy_single_all_journalName_dict,fp)\n",
    "    with gzip.open(os.path.join(data_folder, 'count_all_journalName_dict.pkl.gz'), 'wb') as fp:\n",
    "        joblib.dump(count_all_journalName_dict,fp)\n",
    "    with gzip.open(os.path.join(data_folder, 'sample_all_journalName_dict.pkl.gz'), 'wb') as fp:\n",
    "        joblib.dump(sample_all_journalName_dict,fp)\n",
    "    with gzip.open(os.path.join(data_folder, 'all_journalName_set.pkl.gz'), 'wb') as fp:\n",
    "        joblib.dump(all_journalName_set,fp)\n",
    "    with gzip.open(os.path.join(data_folder, 'all_titles_dict.pkl.gz'), 'wb') as fp:\n",
    "        joblib.dump(all_titles_dict,fp)\n",
    "    with gzip.open(os.path.join(data_folder, 'all_journal_dict.pkl.gz'), 'wb') as fp:\n",
    "        joblib.dump(all_journal_dict,fp)\n",
    "    with gzip.open(os.path.join(data_folder, 'all_fieldsOfStudy_dict.pkl.gz'), 'wb') as fp:\n",
    "        joblib.dump(all_fieldsOfStudy_dict,fp)\n",
    "    with gzip.open(os.path.join(data_folder, 'all_year_volume_issue_firstPage_dict.pkl.gz'), 'wb') as fp:\n",
    "        joblib.dump(all_year_volume_issue_firstPage_dict,fp)\n",
    "    with gzip.open(os.path.join(data_folder, 'all_paperId_by_journal_by_first_fieldsOfStudy.pkl.gz'), 'wb') as fp:\n",
    "        joblib.dump(all_paperId_by_journal_by_first_fieldsOfStudy,fp)\n",
    "    with gzip.open(os.path.join(data_folder, 'all_paperId_by_author.pkl.gz'), 'wb') as fp:\n",
    "        joblib.dump(all_paperId_by_author,fp)\n",
    "    with gzip.open(os.path.join(data_folder, 'count_papers_by_author.pkl.gz'), 'wb') as fp:\n",
    "        joblib.dump(count_papers_by_author,fp)\n",
    "    with gzip.open(os.path.join(data_folder, 'count_papers_by_author_by_first_fieldsOfStudy.pkl.gz'), 'wb') as fp:\n",
    "        joblib.dump(count_papers_by_author_by_first_fieldsOfStudy,fp)\n",
    "    print('Dumped all collected data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a361c4da-1f67-4af4-8c41-589c5423f59b",
   "metadata": {},
   "source": [
    "Remap each word in the dataset into increasing integer indexesconfidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dce5c8ec-ae40-40be-858c-9064976b5926",
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open(os.path.join(data_folder, 'all_titles_dict.pkl.gz'), 'rb') as fp:\n",
    "        all_titles_dict = joblib.load(fp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a949646-c1d9-4922-ac97-0e79b0b38a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing word2index and word2stem mapping...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74ad27f1fd6a4f5ba21a85b7d4a37092",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/130499129 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dumped mappings\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    raise KeyError\n",
    "    print('Trying to load word2index and word2stem mapping...')\n",
    "    with gzip.open(os.path.join(data_folder, 'word2index.pkl.gz'), 'rb') as fp:\n",
    "        word2index = joblib.load(fp)\n",
    "    with gzip.open(os.path.join(data_folder, 'word2stem.pkl.gz'), 'rb') as fp:\n",
    "        word2stem = joblib.load(fp)\n",
    "    print('Loaded word2index and word2stem mapping')\n",
    "except:\n",
    "    print('Computing word2index and word2stem mapping...')\n",
    "    word2index = {}\n",
    "    max_index = 0\n",
    "    word2stem = {}\n",
    "    stem2index = {}\n",
    "    max_stem_index = 0\n",
    "    for title in tqdm(all_titles_dict.values()):\n",
    "#         language_prediction = cld3.get_language(title) # technically now they are all english titles, so no need for this\n",
    "#         if language_prediction.probability > 0.5: # probability > 0.99: # is_reliable == True:\n",
    "#             language = language_prediction.language # This gives a iso639 code\n",
    "#             language = languages.get(alpha2=language).name # get the full name of the language from its iso639 code\n",
    "#         else:\n",
    "#             print(title,language_prediction, language)\n",
    "#             break\n",
    "        words = lemmatize(title)\n",
    "        snow_stemmer = SnowballStemmer(language=language.lower())\n",
    "        for word in words:\n",
    "            if word not in word2index:\n",
    "                word2index[word] = max_index\n",
    "                max_index += 1\n",
    "                stem = snow_stemmer.stem(word)\n",
    "                word2stem[word] = stem\n",
    "                if stem not in stem2index:\n",
    "                    stem2index[stem] = max_stem_index\n",
    "                    max_stem_index += 1\n",
    "    # Dump\n",
    "    with gzip.open(os.path.join(data_folder, 'word2index.pkl.gz'), 'wb') as fp:\n",
    "        joblib.dump(word2index,fp)\n",
    "    with gzip.open(os.path.join(data_folder, 'word2stem.pkl.gz'), 'wb') as fp:\n",
    "        joblib.dump(word2stem,fp)\n",
    "    with gzip.open(os.path.join(data_folder, 'stem2index.pkl.gz'), 'wb') as fp:\n",
    "        joblib.dump(stem2index,fp)\n",
    "    print('Dumped mappings')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586ba1ff-01cc-447d-b51b-09f0440d24d0",
   "metadata": {},
   "source": [
    "# Find most important journals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6580eb0d-e6ea-4ba3-a609-771394d65cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_fieldsOfStudy_tuples_df = {}\n",
    "for fieldsOfStudy in all_fieldsOfStudy_tuples_set:\n",
    "    tmp_df = pd.DataFrame.from_dict(count_by_fieldsOfStudy_tuple_all_journalName_dict[fieldsOfStudy],orient=\"index\",columns=[fieldsOfStudy])\n",
    "    all_fieldsOfStudy_tuples_df[fieldsOfStudy] = tmp_df[pd.notna(tmp_df)][fieldsOfStudy].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87851d8-d33d-4d09-ba6b-a24539f3463a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_fieldsOfStudy_tuples_df[fieldsOfStudy].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d91c343-c0e5-4a54-bc08-e5cc75e7f3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_fieldsOfStudy_couples_df = {}\n",
    "for fieldsOfStudy in all_fieldsOfStudy_couples_set:\n",
    "    tmp_df = pd.DataFrame.from_dict(count_by_fieldsOfStudy_couple_all_journalName_dict[fieldsOfStudy],orient=\"index\",columns=[fieldsOfStudy])\n",
    "    all_fieldsOfStudy_couples_df[fieldsOfStudy] = tmp_df[pd.notna(tmp_df)][fieldsOfStudy].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbd256d-f678-4d8e-b24a-a6af30cc1865",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_fieldsOfStudy_couples_df[fieldsOfStudy].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93924548-8a1a-4fc5-9f4e-316cc7d4770c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_fieldsOfStudy_singles_df = {}\n",
    "for fieldsOfStudy in all_fieldsOfStudy_singles_set:\n",
    "    tmp_df = pd.DataFrame.from_dict(count_by_fieldsOfStudy_single_all_journalName_dict[fieldsOfStudy],orient=\"index\",columns=[fieldsOfStudy])\n",
    "    all_fieldsOfStudy_singles_df[fieldsOfStudy] = tmp_df[pd.notna(tmp_df)][fieldsOfStudy].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4346a8b-1137-4569-a088-b53b2b0fe6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_fieldsOfStudy_singles_df[fieldsOfStudy].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968eb170-5154-4d83-acd4-b5a66261b089",
   "metadata": {},
   "outputs": [],
   "source": [
    "fieldsOfStudy = sorted(all_fieldsOfStudy_singles_set)[0]\n",
    "df = pd.DataFrame.from_dict(count_by_fieldsOfStudy_single_all_journalName_dict[fieldsOfStudy],orient=\"index\",columns=[fieldsOfStudy])\n",
    "tmp_df2 = df.copy()\n",
    "for fieldsOfStudy in sorted(all_fieldsOfStudy_singles_set)[1:]:\n",
    "    tmp_df = pd.DataFrame.from_dict(count_by_fieldsOfStudy_single_all_journalName_dict[fieldsOfStudy],orient=\"index\",columns=[fieldsOfStudy])\n",
    "    df = df.append(tmp_df,sort=True)\n",
    "all_fields_df = df.sum(axis=1,numeric_only=True).sort_values(ascending=False)\n",
    "all_fields_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372ff2f8-b8d8-4c64-b377-73764c6b5f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df = pd.DataFrame.from_dict(count_all_journalName_dict,orient=\"index\",columns=[\"All fields\"])\n",
    "all_fields_df = tmp_df[pd.notna(tmp_df)][\"All fields\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cca48bd-e480-40dc-8a85-71a2aa03b562",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_fields_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334d355d-b450-4a8e-b6b2-5840da0514a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (journalName, venue) in all_fields_df.head(10).index:\n",
    "    print(f'---------- {journalName} ----------')\n",
    "    print('\\n')\n",
    "    print('ID:\\t',sample_all_journalName_dict[(journalName, venue)]['id'])\n",
    "    print('TITLE:\\t',sample_all_journalName_dict[(journalName, venue)]['title'])\n",
    "    print('YEAR:\\t',sample_all_journalName_dict[(journalName, venue)]['year'])\n",
    "    print('journalVolume:\\t',sample_all_journalName_dict[(journalName, venue)]['journalVolume'])\n",
    "    print('journalPages:\\t',sample_all_journalName_dict[(journalName, venue)]['journalPages'])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16abc94-72b0-420d-99e5-6c480de644a3",
   "metadata": {},
   "source": [
    "# Preparing list of titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06c72a2-002e-4ef1-84bb-6070a3a6d8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(os.path.join(data_folder,'all_fieldsOfStudy.tsv'), 'r', newline='\\n') as fp:\n",
    "        tsv_output = csv.reader(fp, delimiter='\\n')\n",
    "        all_fieldsOfStudy = [] \n",
    "        for _ in tsv_output:\n",
    "            all_fieldsOfStudy.append(_[0])\n",
    "except:\n",
    "    all_fieldsOfStudy = sorted(all_fieldsOfStudy_singles_set)\n",
    "    with open(os.path.join(data_folder,'all_fieldsOfStudy.tsv'), 'w', newline='\\n') as fp:\n",
    "        tsv_output = csv.writer(fp, delimiter='\\n')\n",
    "        tsv_output.writerow(all_fieldsOfStudy)\n",
    "print(all_fieldsOfStudy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef48b01-a220-414f-a886-25bf6dbe3c09",
   "metadata": {},
   "source": [
    "## By field of study and journal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb377bd-6e10-47d8-a21c-77aa25cf3ddf",
   "metadata": {},
   "source": [
    "For each fields of study, take the 100 first journals based on the number of papers in that field and journal. \n",
    "\n",
    "Then, for each field and journal, create a list of tokenized words from the time-ordered list of titles, and save each list, where each word is remapped into an index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "062b3965-ae49-496b-8d85-617e190aab8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.makedirs(os.path.join(data_folder,'fieldsOfStudy_original'), exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "827cf90e-9d1e-4a11-9034-13d66ab98a65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3347ebd230f14bcfa79958dc1c66e768",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Art\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79896afc628c478fb8615f43fde07888",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Biology\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe324d147feb4efea9c8958a9f2a617c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Business\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fb038ad4b7f4ceab5fe194fc0ec6762",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Chemistry\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5905a3b5edbc41aab0d52060f1969cd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Computer Science\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "309096874e6b476d8674fc403cc636a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Economics\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "242cf158f77646e8ac4d86aaa66a7b4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Engineering\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70587075071346f4b6813b440c7bb1d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Environmental Science\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5763170f70a48838bc2ad0c4cce8e29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Geography\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a230e42c14f4e7fbc77dd03c94c8e91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Geology\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29cfdcc2d9734f1cb65c1eebcc7c2b25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting History\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9ce68d7832048a186162c22563632a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Materials Science\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed1538602f3a4b269226b7a6581b9823",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Mathematics\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a50b9fbc7b4e4363a97259609add1ad7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Medicine\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e19719110bf24be59416f5740f369de3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Philosophy\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41ce192f0f8f449a80c19894939d5471",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Physics\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85bdffd354484accb71f32d9927fe20d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Political Science\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "038df800fbc94eecab75aaaca486fa7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Psychology\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aab354628f7b4f4f9b3516891c788df6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Sociology\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43f29c1bafbc46c1aeff9334f1cfbe87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "journals_fieldsOfStudy_name_dict = {}\n",
    "num_journals = 1000\n",
    "for fieldOfStudy in tqdm(all_fieldsOfStudy):\n",
    "    print('Starting', fieldOfStudy)\n",
    "    journals = list(all_fieldsOfStudy_singles_df[fieldOfStudy].index[:num_journals])\n",
    "    fieldsOfStudy_original_folder = os.path.join(data_folder,'journals_fieldsOfStudy_original',fieldOfStudy)\n",
    "    os.makedirs(fieldsOfStudy_original_folder, exist_ok = True)\n",
    "    fieldsOfStudy_folder = os.path.join(data_folder,'journals_fieldsOfStudy',fieldOfStudy)\n",
    "    os.makedirs(fieldsOfStudy_folder, exist_ok = True)\n",
    "    fieldsOfStudy_stems_folder = os.path.join(data_folder,'journals_fieldsOfStudy_stems',fieldOfStudy)\n",
    "    os.makedirs(fieldsOfStudy_stems_folder, exist_ok = True)\n",
    "    papers_with_no_year = []\n",
    "    ordered_journal_index = -1 \n",
    "    for (journal,venue) in tqdm(journals):\n",
    "        ordered_journal_index += 1\n",
    "        # find all papers of the journal in the considered fieldOfStudy\n",
    "        list_to_sort = []\n",
    "        for paper_id in all_paperId_by_journal_by_first_fieldsOfStudy[fieldOfStudy][(journal,venue)]:\n",
    "            if all_year_volume_issue_firstPage_dict[paper_id][0] is not None:\n",
    "                list_to_sort.append(all_year_volume_issue_firstPage_dict[paper_id] + tuple([paper_id]))\n",
    "#             else:\n",
    "#                 papers_with_no_year.append(paper_id)\n",
    "#         # see if there is any other paper with the same volume and copy its year\n",
    "#         if len(papers_with_no_year) > 0:\n",
    "#             print('Some paper do not have year key, trying to solve')\n",
    "#             for paper_id in tqdm(papers_with_no_year):\n",
    "#                 volume = all_year_volume_issue_firstPage_dict[paper_id][1]\n",
    "#                 issue = all_year_volume_issue_firstPage_dict[paper_id][2]\n",
    "#                 firstPage = all_year_volume_issue_firstPage_dict[paper_id][3]\n",
    "#                 if volume != -1:\n",
    "#                     for tuple_paper in list_to_sort:\n",
    "#                         if tuple_paper[1] == volume:\n",
    "#                             list_to_sort.append((tuple_paper[0], volume, issue, firstPage, paper_id))\n",
    "        # sort it using year,volume,issue,firstPage,paper_id\n",
    "        sorted_list = sorted(list_to_sort)\n",
    "        # create list of titles\n",
    "        ordered_list_of_words = []\n",
    "        for tmp_tuple in sorted_list:\n",
    "            paper_id = tmp_tuple[-1]\n",
    "            ordered_list_of_words += lemmatize(all_titles_dict[paper_id])\n",
    "        with open(os.path.join(fieldsOfStudy_original_folder,f'{ordered_journal_index}.tsv'), 'w', newline='\\n') as fp:\n",
    "            tsv_output = csv.writer(fp, delimiter='\\n')\n",
    "            tsv_output.writerow(ordered_list_of_words)\n",
    "        indexed_ordered_list_of_words = [word2index[key] for key in ordered_list_of_words]\n",
    "        with open(os.path.join(fieldsOfStudy_folder,f'{ordered_journal_index}.tsv'), 'w', newline='\\n') as fp:\n",
    "            tsv_output = csv.writer(fp, delimiter='\\n')\n",
    "            tsv_output.writerow(indexed_ordered_list_of_words)\n",
    "        indexed_ordered_list_of_stems = [stem2index[word2stem[key]] for key in ordered_list_of_words]\n",
    "        with open(os.path.join(fieldsOfStudy_stems_folder,f'{ordered_journal_index}.tsv'), 'w', newline='\\n') as fp:\n",
    "            tsv_output = csv.writer(fp, delimiter='\\n')\n",
    "            tsv_output.writerow(indexed_ordered_list_of_stems)\n",
    "    journals_fieldsOfStudy_name_dict[fieldOfStudy] = journals.copy()\n",
    "    with open(os.path.join(data_folder,f'journals_fieldsOfStudy_name.pkl'), 'wb') as fp:\n",
    "        joblib.dump(journals_fieldsOfStudy_name_dict, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9761d8e-612e-4c20-a449-55b10ae46f0e",
   "metadata": {},
   "source": [
    "## By authors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa5eb49-c8b8-4a8e-971c-1a0dc61c5210",
   "metadata": {},
   "source": [
    "Take the 10000 first authors based on their number of papers. \n",
    "\n",
    "Then, for each of them, create a list of tokenized words from the time-ordered list of titles, and save each list, where each word is remapped into an index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "556c87ce-5193-4806-844f-87810f18f587",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_papers_by_author_df = pd.DataFrame.from_dict(count_papers_by_author,orient=\"index\",columns=['no_papers'])\n",
    "ordered_count_papers_by_author_df = count_papers_by_author_df[pd.notna(count_papers_by_author_df)].no_papers.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8e498863-b2f5-46f1-a528-8d3876cde7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_authors = 10000\n",
    "authors = list(ordered_count_papers_by_author_df.index[:num_authors])\n",
    "authors_original_folder = os.path.join(data_folder,'authors_original')\n",
    "os.makedirs(authors_original_folder, exist_ok = True)\n",
    "authors_folder = os.path.join(data_folder,'authors')\n",
    "os.makedirs(authors_folder, exist_ok = True)\n",
    "authors_stems_folder = os.path.join(data_folder,'authors_stems')\n",
    "os.makedirs(authors_stems_folder, exist_ok = True)\n",
    "with open(os.path.join(data_folder,f'authors_name.pkl'), 'wb') as fp:\n",
    "    joblib.dump(authors, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ea6ba104-ba26-42a9-a47b-407a506b50f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9898cf3c237746d387ffd295bd40837f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for ordered_author_index,author in tqdm(enumerate(authors)):\n",
    "    # find all papers of the author\n",
    "    list_to_sort = []\n",
    "    for paper_id in all_paperId_by_author[author]:\n",
    "        info = all_year_volume_issue_firstPage_dict[paper_id]\n",
    "        if info[0] is not None:\n",
    "            list_to_sort.append(info + tuple([paper_id]))\n",
    "    # sort it using year,volume,issue,firstPage,paper_id\n",
    "    sorted_list = sorted(list_to_sort)\n",
    "    # create list of titles\n",
    "    ordered_list_of_words = []\n",
    "    for tmp_tuple in sorted_list:\n",
    "        paper_id = tmp_tuple[-1]\n",
    "        ordered_list_of_words += lemmatize(all_titles_dict[paper_id])\n",
    "    with open(os.path.join(authors_original_folder,f'{ordered_author_index}.tsv'), 'w', newline='\\n') as fp:\n",
    "        tsv_output = csv.writer(fp, delimiter='\\n')\n",
    "        tsv_output.writerow(ordered_list_of_words)\n",
    "    indexed_ordered_list_of_words = [word2index[key] for key in ordered_list_of_words]\n",
    "    with open(os.path.join(authors_folder,f'{ordered_author_index}.tsv'), 'w', newline='\\n') as fp:\n",
    "        tsv_output = csv.writer(fp, delimiter='\\n')\n",
    "        tsv_output.writerow(indexed_ordered_list_of_words)\n",
    "    indexed_ordered_list_of_stems = [stem2index[word2stem[key]] for key in ordered_list_of_words]\n",
    "    with open(os.path.join(authors_stems_folder,f'{ordered_author_index}.tsv'), 'w', newline='\\n') as fp:\n",
    "        tsv_output = csv.writer(fp, delimiter='\\n')\n",
    "        tsv_output.writerow(indexed_ordered_list_of_stems)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b638e8c7-b2f7-4d36-931d-d033f33efc47",
   "metadata": {},
   "source": [
    "## By field of study and author"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329e2d97-bf68-4538-a1de-70a143deae52",
   "metadata": {},
   "source": [
    "For each fields of study, take the 1000 first authors based on their number of papers in that field. \n",
    "\n",
    "Then, for each of these authors, create a list of tokenized words from the time-ordered list of titles they have written (considering all papers, including other fields), and save each list, where each word is remapped into an index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "16164b80-d077-4431-8add-b9db1069aec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3eaaffc7e514504b802bd557bb2af5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Art\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46c1885440bb4872a506ca36ffa5a068",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Biology\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f88cee65e6974b1db9685ebc537fee64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Business\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87911525fe26469fa169409d976a3490",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Chemistry\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d901cb99fc8c4e9f967d0ff39293f8e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Computer Science\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "055e94e3ef6e4ecbba4c8ab4180c4261",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Economics\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c01295f6561a49cb828722ae44e50556",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Engineering\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95e86ac3818a4e37a4bb43f4b04b23b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Environmental Science\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2cd23f6a3964e139008e83bf15a51b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Geography\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef83c791a7b04a3185294a80e276455b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Geology\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cea1764931646e6a9c72a66e59a870e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting History\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2684493d8c8f4da4a5281d790f1ed327",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Materials Science\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5091190db55b45bba6e207b34037f385",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Mathematics\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca8d7a594d1e41fc88fc1a089f07a7f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Medicine\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eebd3a422c84ce3acbbac5fe2485741",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Philosophy\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b37737301d540a29180bd5e6aac7ead",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Physics\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c3b29beccc34149bda9267d2fcb3762",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Political Science\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e91aa34b77184aae9af7c84f8bea7029",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Psychology\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5277194f3bc4360ae75d743550309a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Sociology\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d34dcb48282b44d18aae6b0e7074c4a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "authors_fieldsOfStudy_name_dict = {}\n",
    "num_authors = 1000\n",
    "for fieldOfStudy in tqdm(all_fieldsOfStudy):\n",
    "    print('Starting', fieldOfStudy)\n",
    "    \n",
    "    tmp_count_papers_by_author_df = pd.DataFrame.from_dict(count_papers_by_author_by_first_fieldsOfStudy[fieldOfStudy],orient=\"index\",columns=['no_papers'])\n",
    "    ordered_tmp_count_papers_by_author_df = count_papers_by_author_df[pd.notna(tmp_count_papers_by_author_df)].no_papers.sort_values(ascending=False)\n",
    "    \n",
    "    authors = list(ordered_tmp_count_papers_by_author_df.index[:num_authors])\n",
    "    authors_original_folder = os.path.join(data_folder,'authors_fieldsOfStudy_original',fieldOfStudy)\n",
    "    os.makedirs(authors_original_folder, exist_ok = True)\n",
    "    authors_folder = os.path.join(data_folder,'authors_fieldsOfStudy',fieldOfStudy)\n",
    "    os.makedirs(authors_folder, exist_ok = True)\n",
    "    authors_stems_folder = os.path.join(data_folder,'authors_fieldsOfStudy_stems',fieldOfStudy)\n",
    "    os.makedirs(authors_stems_folder, exist_ok = True)\n",
    "    for ordered_author_index,author in tqdm(enumerate(authors)):\n",
    "        # find all papers of the author\n",
    "        list_to_sort = []\n",
    "        for paper_id in all_paperId_by_author[author]:\n",
    "            info = all_year_volume_issue_firstPage_dict[paper_id]\n",
    "            if info[0] is not None:\n",
    "                list_to_sort.append(info + tuple([paper_id]))\n",
    "        # sort it using year,volume,issue,firstPage,paper_id\n",
    "        sorted_list = sorted(list_to_sort)\n",
    "        # create list of titles\n",
    "        ordered_list_of_words = []\n",
    "        for tmp_tuple in sorted_list:\n",
    "            paper_id = tmp_tuple[-1]\n",
    "            ordered_list_of_words += lemmatize(all_titles_dict[paper_id])\n",
    "        with open(os.path.join(authors_original_folder,f'{ordered_author_index}.tsv'), 'w', newline='\\n') as fp:\n",
    "            tsv_output = csv.writer(fp, delimiter='\\n')\n",
    "            tsv_output.writerow(ordered_list_of_words)\n",
    "        indexed_ordered_list_of_words = [word2index[key] for key in ordered_list_of_words]\n",
    "        with open(os.path.join(authors_folder,f'{ordered_author_index}.tsv'), 'w', newline='\\n') as fp:\n",
    "            tsv_output = csv.writer(fp, delimiter='\\n')\n",
    "            tsv_output.writerow(indexed_ordered_list_of_words)\n",
    "        indexed_ordered_list_of_stems = [stem2index[word2stem[key]] for key in ordered_list_of_words]\n",
    "        with open(os.path.join(authors_stems_folder,f'{ordered_author_index}.tsv'), 'w', newline='\\n') as fp:\n",
    "            tsv_output = csv.writer(fp, delimiter='\\n')\n",
    "            tsv_output.writerow(indexed_ordered_list_of_stems)\n",
    "    authors_fieldsOfStudy_name_dict[fieldOfStudy] = authors.copy()\n",
    "    with open(os.path.join(data_folder,f'authors_fieldsOfStudy_name.pkl'), 'wb') as fp:\n",
    "        joblib.dump(authors_fieldsOfStudy_name_dict, fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
